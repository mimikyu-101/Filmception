{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21bd060",
   "metadata": {},
   "source": [
    "### plot_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b6db62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\nadee\\AppData\\Local\\Temp\\ipykernel_16496\\707422475.py:20: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df = pd.read_csv('archive\\MovieSummaries\\plot_summaries.txt', sep='\\t', header=None, names=['id', 'summary'])\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                            summary  \\\n",
      "0  movie_ID summary                                                NaN   \n",
      "1          23890098  Shlykov, a hard-working taxi driver and Lyosha...   \n",
      "2          31186339  The nation of Panem consists of a wealthy Capi...   \n",
      "3          20663735  Poovalli Induchoodan  is sentenced for six yea...   \n",
      "4           2231378  The Lemon Drop Kid , a New York City swindler,...   \n",
      "\n",
      "                                       clean_summary  \n",
      "0                                                     \n",
      "1  shlykov hardworking taxi driver lyosha saxopho...  \n",
      "2  nation panem consists wealthy capitol twelve p...  \n",
      "3  poovalli induchoodan sentenced six year prison...  \n",
      "4  lemon drop kid new york city swindler illegall...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Required for newer versions of NLTK\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load TXT file assuming tab-separated values\n",
    "df = pd.read_csv('archive\\MovieSummaries\\plot_summaries.txt', sep='\\t', header=None, names=['id', 'summary'])\n",
    "\n",
    "# Preprocessing function with type and null check\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['clean_summary'] = df['summary'].apply(preprocess_text)\n",
    "\n",
    "# Save or inspect the output\n",
    "df.to_csv('preprocessed_plot_summaries.txt', sep='\\t', index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c054da",
   "metadata": {},
   "source": [
    "### movie.metadata.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7de8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\nadee\\AppData\\Local\\Temp\\ipykernel_16496\\3494771454.py:19: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df = pd.read_csv('archive\\MovieSummaries\\movie.metadata.tsv', sep='\\t', header=None)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                clean_title  \\\n",
      "0    975900                                  ghost mar   \n",
      "1   3196793  getting away murder jonben ramsey mystery   \n",
      "2  28463795                                brun bitter   \n",
      "3   9363483                                  white eye   \n",
      "4    261236                                woman flame   \n",
      "\n",
      "                                          genre_list  \n",
      "0  [Thriller, Science Fiction, Horror, Adventure,...  \n",
      "1   [Mystery, Biographical film, Drama, Crime Drama]  \n",
      "2                             [Crime Fiction, Drama]  \n",
      "3  [Thriller, Erotic thriller, Psychological thri...  \n",
      "4                                            [Drama]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the TSV file\n",
    "df = pd.read_csv('archive\\MovieSummaries\\movie.metadata.tsv', sep='\\t', header=None)\n",
    "df.columns = ['id', 'fb_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    "\n",
    "# Function to extract genre names from dictionary-like strings\n",
    "def extract_dict_values(cell):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(cell)\n",
    "        return list(parsed.values())\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Apply genre extraction\n",
    "df['genre_list'] = df['genres'].apply(extract_dict_values)\n",
    "\n",
    "# Full preprocessing for the movie title\n",
    "def preprocess_title(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply to title\n",
    "df['clean_title'] = df['title'].apply(preprocess_title)\n",
    "\n",
    "# Save only useful columns\n",
    "df[['id', 'clean_title', 'genre_list']].to_csv('preprocessed_movie_metadata.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Preview\n",
    "print(df[['id', 'clean_title', 'genre_list']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9d798",
   "metadata": {},
   "source": [
    "### character.metadata.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036db2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:30: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\nadee\\AppData\\Local\\Temp\\ipykernel_16496\\185106807.py:30: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df = pd.read_csv('archive\\MovieSummaries\\character.metadata.tsv', sep='\\t', header=None)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id        clean_character_name    clean_actor_name\n",
      "0    975900                    akooshay      wanda de jesus\n",
      "1    975900  lieutenant melanie ballard  natasha henstridge\n",
      "2    975900         desolation williams            ice cube\n",
      "3    975900          sgt jericho butler       jason statham\n",
      "4    975900             bashira kincaid         clea duvall\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK is ready\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Init tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load TSV file\n",
    "df = pd.read_csv('archive\\MovieSummaries\\character.metadata.tsv', sep='\\t', header=None)\n",
    "df.columns = [\n",
    "    'movie_id', 'fb_movie_id', 'release_date', 'character_name', 'actor_dob',\n",
    "    'actor_gender', 'actor_height', 'actor_ethnicity', 'actor_name',\n",
    "    'actor_age_at_release', 'fb_char_actor_map_id', 'fb_character_id', 'fb_actor_id'\n",
    "]\n",
    "\n",
    "# Preprocess relevant columns\n",
    "df['clean_character_name'] = df['character_name'].apply(preprocess_text)\n",
    "df['clean_actor_name'] = df['actor_name'].apply(preprocess_text)\n",
    "\n",
    "# Save relevant columns\n",
    "df[['movie_id', 'clean_character_name', 'clean_actor_name']].to_csv('preprocessed_character_metadata.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Preview\n",
    "print(df[['movie_id', 'clean_character_name', 'clean_actor_name']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb570f",
   "metadata": {},
   "source": [
    "### tvtropes.cluster.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420fcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id              clean_trope                 clean_char  \\\n",
      "0   /m/0jy9q0  absent minded professor  professor philip brainard   \n",
      "1  /m/02vchl3  absent minded professor         professor keenbean   \n",
      "2   /m/0k6fkc  absent minded professor          dr reinhardt lane   \n",
      "3   /m/0k6_br  absent minded professor          dr harold medford   \n",
      "4   /m/0k3rhh  absent minded professor             daniel jackson   \n",
      "\n",
      "   clean_movie      clean_actor  \n",
      "0      flubber   robin williams  \n",
      "1  richie rich  michael mcshane  \n",
      "2       shadow     ian mckellen  \n",
      "3                  edmund gwenn  \n",
      "4     stargate     james spader  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK is ready\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Init tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load and process the file line-by-line\n",
    "records = []\n",
    "with open('archive/MovieSummaries/tvtropes.clusters.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "        trope, data_str = line.strip().split('\\t', 1)\n",
    "        try:\n",
    "            data = json.loads(data_str.replace(\"'\", '\"'))  # Ensure valid JSON\n",
    "            record = {\n",
    "                'original_trope': trope,\n",
    "                'original_char': data.get('char', ''),\n",
    "                'original_movie': data.get('movie', ''),\n",
    "                'original_actor': data.get('actor', ''),\n",
    "                'id': data.get('id', '')\n",
    "            }\n",
    "            # Preprocess fields\n",
    "            record['clean_trope'] = preprocess_text(trope)\n",
    "            record['clean_char'] = preprocess_text(data.get('char', ''))\n",
    "            record['clean_movie'] = preprocess_text(data.get('movie', ''))\n",
    "            record['clean_actor'] = preprocess_text(data.get('actor', ''))\n",
    "            records.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # Skip malformed rows\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save to TSV\n",
    "df[['id', 'clean_trope', 'clean_char', 'clean_movie', 'clean_actor']].to_csv('preprocessed_tvtropes_clusters.txt', sep='\\t', index=False)\n",
    "\n",
    "# Preview\n",
    "print(df[['id', 'clean_trope', 'clean_char', 'clean_movie', 'clean_actor']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b260c7",
   "metadata": {},
   "source": [
    "### name.cluster.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05c194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   original_name         id     clean_name\n",
      "0  Stuart Little  /m/0k3w9c  stuart little\n",
      "1  Stuart Little  /m/0k3wcx  stuart little\n",
      "2  Stuart Little  /m/0k3wbn  stuart little\n",
      "3       John Doe  /m/0jyg35       john doe\n",
      "4       John Doe  /m/0k2_zn       john doe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Read and preprocess the file\n",
    "processed_data = []\n",
    "with open('archive/MovieSummaries/name.clusters.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "        name, id_ = line.strip().split('\\t', 1)\n",
    "        processed_data.append({\n",
    "            'original_name': name,\n",
    "            'id': id_,\n",
    "            'clean_name': preprocess_text(name)\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Save to TSV\n",
    "df[['id', 'clean_name']].to_csv('preprocessed_name_clusters.txt', sep='\\t', index=False)\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cde1e0",
   "metadata": {},
   "source": [
    "### metadata extraction / genre classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790d84f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    wiki_id                                            summary  \\\n",
      "0  23890098  shlykov hard working taxi driver lyosha saxoph...   \n",
      "1  31186339  nation panem consists wealthy capitol twelve p...   \n",
      "2  20663735  poovalli induchoodan sentenced six year prison...   \n",
      "3   2231378  lemon drop kid new york city swindler illegall...   \n",
      "4    595909  seventh day adventist church pastor michael ch...   \n",
      "\n",
      "                                              genres  \n",
      "0                              [Drama, World cinema]  \n",
      "1  [Action/Adventure, Science Fiction, Action, Dr...  \n",
      "2                [Musical, Action, Drama, Bollywood]  \n",
      "3                         [Screwball comedy, Comedy]  \n",
      "4  [Crime Fiction, Drama, Docudrama, World cinema...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK components\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setup for text cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # Remove special chars/numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load and preprocess plot summaries\n",
    "plot_df = pd.read_csv('archive/MovieSummaries/plot_summaries.txt', sep='\\t', names=['wiki_id', 'summary'])\n",
    "plot_df['summary'] = plot_df['summary'].apply(preprocess_text)\n",
    "\n",
    "# Load and extract genres from movie metadata\n",
    "metadata_df = pd.read_csv('archive/MovieSummaries/movie.metadata.tsv', sep='\\t', header=None)\n",
    "metadata_df = metadata_df[[0, 8]]  # Keep only Wikipedia ID and genres\n",
    "metadata_df.columns = ['wiki_id', 'genres']\n",
    "\n",
    "# Parse genre stringified dicts\n",
    "def extract_genres(genre_dict_str):\n",
    "    try:\n",
    "        genre_dict = ast.literal_eval(genre_dict_str)\n",
    "        if isinstance(genre_dict, dict):\n",
    "            return list(genre_dict.values())  # Extract only genre names\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "metadata_df['genres'] = metadata_df['genres'].apply(extract_genres)\n",
    "\n",
    "# Merge on Wikipedia Movie ID\n",
    "merged_df = pd.merge(plot_df, metadata_df, on='wiki_id', how='inner')\n",
    "\n",
    "# Remove entries with empty summaries or genres\n",
    "merged_df = merged_df[(merged_df['summary'].str.strip() != '') & (merged_df['genres'].map(len) > 0)]\n",
    "\n",
    "# Save to final TSV\n",
    "merged_df.to_csv('cleaned_movie_data_for_genre_classification.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Preview\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fdba4",
   "metadata": {},
   "source": [
    "### train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e94ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (33434, 365)\n",
      "Test set size: (8359, 365)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv('cleaned_movie_data_for_genre_classification.tsv', sep='\\t')\n",
    "\n",
    "# Convert string genre list to actual list\n",
    "import ast\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Binarize multi-label genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_matrix = mlb.fit_transform(df['genres'])\n",
    "genre_df = pd.DataFrame(genre_matrix, columns=mlb.classes_)\n",
    "\n",
    "# Combine with original data\n",
    "df_combined = pd.concat([df[['wiki_id', 'summary']], genre_df], axis=1)\n",
    "\n",
    "# Train-test split (stratification not directly supported for multi-label, but we shuffle)\n",
    "train_df, test_df = train_test_split(df_combined, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Save splits\n",
    "train_df.to_csv('train_genre_dataset.tsv', sep='\\t', index=False)\n",
    "test_df.to_csv('test_genre_dataset.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Optional: Save label encoder for later inverse-transform\n",
    "import pickle\n",
    "with open('genre_mlb.pkl', 'wb') as f:\n",
    "    pickle.dump(mlb, f)\n",
    "\n",
    "# Preview\n",
    "print(\"Training set size:\", train_df.shape)\n",
    "print(\"Test set size:\", test_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
